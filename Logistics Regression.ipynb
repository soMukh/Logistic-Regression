{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673019e4",
   "metadata": {},
   "source": [
    "Theoretical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6abeafd",
   "metadata": {},
   "source": [
    "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
    "\n",
    "-> Logistic Regression is used for classification tasks where the target variable is categorical — such as predicting whether an email is spam or not. Instead of predicting a raw numerical value, it predicts the probability that a given input belongs to a certain class. It uses the sigmoid(logistic) function to squeeze the output between 0 and 1, which can then be interpreted as a probability. A threshold(commonly 0.5) is used to assign the final class label. Logistic Regression optimizes the model using log loss(cross-entropy) instead of MSE.\n",
    "\n",
    "Difference between Linear Regression and Logistic Regression:\n",
    "\n",
    "Logistic Regression:\n",
    "\n",
    "a. Used for: Classification(e.g., spam vs. not spam)\n",
    "\n",
    "b. Output: Probability between 0 and 1\n",
    "\n",
    "c. Function: Uses sigmoid function\n",
    "\n",
    "d. Cost Function: Log loss(cross-entropy)\n",
    "\n",
    "e. Example: Predicting if a customer will default on a loan\n",
    "\n",
    "Linear Regression:\n",
    "\n",
    "a. Used for: Regression(e.g., predicting prices)\n",
    "\n",
    "b. Output: Any real number\n",
    "\n",
    "c. Function: Direct linear equation\n",
    "\n",
    "d. Cost Function: Mean squared error(MSE)\n",
    "\n",
    "e. Example: Predicting house prices\n",
    "\n",
    "2. What is the mathematical equation of Logistic Regression?\n",
    "\n",
    "-> The mathematical equation of Logistic Regression is: \n",
    "\n",
    "P(y=1 | x) = `1/(1+exp(-(β0+β1*x1+β2*x2+...+βn*xn)))`\n",
    "\n",
    "Where:\n",
    "\n",
    "P(y=1 | x) : Probability that the target y is 1 given input features x\n",
    "\n",
    "β0 : Intercept(bias term)\n",
    "\n",
    "β1...βn : Coefficients for features x1 to xn\n",
    "\n",
    "x1...xn : Input feature values\n",
    "\n",
    "exp : Exponential function(e^x)\n",
    "\n",
    "3. Why do we use the Sigmoid function in Logistic Regression?\n",
    "\n",
    "-> We use the sigmoid function in Logistic Regression because it converts any real-valued input into a probability value between 0 and 1, which is ideal for classification tasks.\n",
    "\n",
    "Reasons for using the Sigmoid Function:\n",
    "\n",
    "a. Probability Output: Logistic regression predicts the probability of an instance belonging to class 1. The sigmoid function naturally maps input values to the (0, 1) interval.\n",
    "\n",
    "σ(z)=1/(1+e^−z)\n",
    "​\n",
    "\n",
    "b. Smooth Decision Boundary: The sigmoid gives a smooth, S-shaped curve, allowing for gradual transitions between classes rather than abrupt ones.\n",
    "\n",
    "c. Differentiable: It's differentiable, which is essential for optimization during training using gradient descent.\n",
    "\n",
    "d. Binary Classification Suitability: The sigmoid's output near 0 and 1 makes it perfect for binary classification — we can threshold the result (e.g., >0.5->class 1, else class 0).\n",
    "\n",
    "e. Interpretability: The output can be interpreted as a confidence score or probability, which is useful in real-world applications (e.g., risk of loan default).\n",
    "\n",
    "4. What is the cost function of Logistic Regression?\n",
    "\n",
    "-> The cost function used in Logistic Regression is called the Log Loss or Binary Cross-Entropy Loss.\n",
    "\n",
    "Logistic Regression Cost Function(Binary Cross-Entropy/Log Loss):\n",
    "\n",
    "For a single training example:\n",
    "\n",
    "Cost=`-[y*log(y_hat)+(1-y)*log(1-y_hat)]`\n",
    "\n",
    "Where:\n",
    "\n",
    "y=Actual class label(0 or 1)\n",
    "\n",
    "y_hat=Predicted probability(sigmoid output)\n",
    "\n",
    "For all m training examples:\n",
    "\n",
    "J(theta)=`(1/m)*Σ[-y(i)*log(y_hat(i))-(1-y(i))*log(1-y_hat(i))]` where i=1 to m\n",
    "\n",
    "This function penalizes incorrect predictions more when the model is confident but wrong.\n",
    "\n",
    "It is convex, making it suitable for optimization using gradient descent.\n",
    "\n",
    "5. What is Regularization in Logistic Regression? Why is it needed?\n",
    "\n",
    "-> Regularization is a technique used to prevent overfitting in machine learning models, including logistic regression. It does so by adding a penalty term to the cost function, which discourages the model from assigning large weights to the features.\n",
    "\n",
    "Regularization is needed because:\n",
    "\n",
    "a. When a model becomes too complex(e.g., high coefficients), it can fit the training data very well but fail to generalize to new, unseen data — this is called overfitting.\n",
    "\n",
    "b. Regularization keeps the model simpler and more generalizable by shrinking the weights.\n",
    "\n",
    "Regularization helps improve generalization by penalizing large weights, thus reducing overfitting. L1(Lasso) and L2(Ridge) are the most common types used in logistic regression.\n",
    "\n",
    "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
    "\n",
    "-> Difference between Lasso, Ridge, and Elastic Net Regression:\n",
    "\n",
    "a. Ridge Regression(L2 Regularization):\n",
    "\n",
    "a.1. Penalty Term: J(θ)=Loss+λ*sum(θ_j^2)\n",
    "\n",
    "a.2. Effect: Shrinks coefficients but does not make them zero — all features are retained, but with smaller weights.\n",
    "\n",
    "a.3. Best For: Cases where many features contribute a little(i.e., all features are relevant, but none strongly dominate).\n",
    "\n",
    "b. Lasso Regression(L1 Regularization):\n",
    "\n",
    "b.1. Penalty Term: J(θ)=Loss+λ*sum(|θ_j|)\n",
    "\n",
    "b.2. Effect: Encourages sparsity — can shrink some coefficients exactly to zero, effectively performing feature selection.\n",
    "\n",
    "b.3. Best For: Situations with high-dimensional data or when only a few features are expected to be important.\n",
    "\n",
    "c. Elastic Net Regression(L1+L2 Regularization):\n",
    "\n",
    "c.1. Penalty Term: `J(θ)=Loss+λ1*sum(|θ_j|)+λ2*sum(θ_j^2)`\n",
    " \n",
    "c.2. Effect: Combines both sparsity (from L1) and shrinkage (from L2). Useful when:\n",
    "\n",
    "a. We have correlated features\n",
    "\n",
    "b. We want both feature selection and stability\n",
    "\n",
    "c.3. Best For: Complex datasets where neither L1 nor L2 alone performs well.\n",
    "\n",
    "7. When should we use Elastic Net instead of Lasso or Ridge?\n",
    "\n",
    "-> We should use Elastic Net when we want to combine the strengths of both Lasso and Ridge, especially in the following situations:\n",
    "\n",
    "a. When We Have Many Correlated Features:\n",
    "\n",
    "a.1. Ridge tends to distribute the weights among correlated features.\n",
    "\n",
    "a.2. Lasso picks one feature and ignores the others(arbitrarily).\n",
    "\n",
    "a.3. Elastic Net does both: it can select features while also sharing weights among correlated ones, making it more stable.\n",
    "\n",
    "b. When Lasso Underperforms:\n",
    "\n",
    "b.1. Lasso can struggle when:\n",
    "\n",
    "b.1.a. The number of features>number of samples\n",
    "\n",
    "b.1.b. Features are highly correlated\n",
    "\n",
    "b.2. Elastic Net helps by adding the L2(ridge) term to stabilize the selection process.\n",
    "\n",
    "c. When We Want Both Sparsity and Regularization:\n",
    "\n",
    "c.1. Lasso gives sparsity(zeroing out features).\n",
    "\n",
    "c.2. Ridge gives better generalization(shrinkage).\n",
    "\n",
    "c.3. Elastic Net gives us a controlled balance of both using l1_ratio.\n",
    "\n",
    "8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
    "\n",
    "-> The regularization parameter λ(lambda) controls the strength of regularization in logistic regression. It determines how much the model should penalize large coefficients.\n",
    "\n",
    "λ affects the model:\n",
    "\n",
    "a. When λ is Large(Strong Regularization):\n",
    "\n",
    "a.1. Coefficients are heavily penalized -> they shrink closer to zero.\n",
    "\n",
    "a.2. Model becomes simpler and less flexible.\n",
    "\n",
    "a.3. Risk of underfitting increases.\n",
    "\n",
    "a.4. Helps reduce overfitting and improves generalization.\n",
    "\n",
    "b. When λ is Small(Weak Regularization):\n",
    "\n",
    "b.1. Penalty is minimal -> coefficients can grow large.\n",
    "\n",
    "b.2. Model becomes more flexible, fits the training data closely.\n",
    "\n",
    "b.3. Risk of overfitting increases if the model is too complex.\n",
    "\n",
    "c. When λ=0(No Regularization):\n",
    "\n",
    "c.1. Equivalent to standard logistic regression without any penalty.\n",
    "\n",
    "c.2. May overfit on noisy or high-dimensional data.\n",
    "\n",
    "9. What are the key assumptions of Logistic Regression?\n",
    "\n",
    "-> While logistic regression is less strict than linear regression, it still relies on some important assumptions for optimal performance:\n",
    "\n",
    "a. Binary or Categorical Outcome:\n",
    "\n",
    "a.1. The dependent variable must be binary(0/1) or categorical(for multiclass logistic).\n",
    "\n",
    "a.2. For binary logistic regression: y∈{0,1}\n",
    "\n",
    "b. Independence of Observations:\n",
    "\n",
    "b.1. Each training example should be independent of the others.\n",
    "\n",
    "b.2. Violations(e.g., repeated measures, time-series) can bias results.\n",
    "\n",
    "c. No or Little Multicollinearity:\n",
    "\n",
    "c.1. Independent variables(features) should not be highly correlated with each other.\n",
    "\n",
    "c.2. High multicollinearity makes coefficient estimates unstable.\n",
    "\n",
    "d. Linearity of Logit:\n",
    "\n",
    "d.1. Logistic regression assumes a linear relationship between the logit of the outcome and the independent variables:\n",
    "\n",
    "log(P/1−P)=β0+β1x1+⋯+βnxn\n",
    "​\n",
    "d.2. The log-odds(not the probability itself) must relate linearly to predictors.\n",
    "\n",
    "e. Large Sample Size:\n",
    "\n",
    "e.1. Logistic regression works best with a large dataset, especially when the outcome is rare.\n",
    "\n",
    "e.2. More data leads to better estimation of probabilities.\n",
    "\n",
    "f. No Extreme Outliers:\n",
    "\n",
    "f.1. Outliers in the independent variables can distort the model.\n",
    "\n",
    "f.2. Should be detected and handled appropriately.\n",
    "\n",
    "10. What are some alternatives to Logistic Regression for classification tasks?\n",
    "\n",
    "-> Some commonly used alternatives to Logistic Regression for classification tasks, especially when logistic regression doesn’t perform well:\n",
    "\n",
    "a. Decision Trees:\n",
    "\n",
    "a.1. Splits data based on feature values to form a tree structure.\n",
    "\n",
    "a.2. Easy to interpret, handles nonlinear relationships.\n",
    "\n",
    "a.3. Can overfit if not pruned.\n",
    "\n",
    "b. Random Forest:\n",
    "\n",
    "b.1. Ensemble of decision trees(bagging).\n",
    "\n",
    "b.2. Reduces overfitting, improves accuracy and robustness.\n",
    "\n",
    "b.3. Handles both linear and nonlinear patterns well.\n",
    "\n",
    "c. Support Vector Machines(SVM):\n",
    "\n",
    "c.1. Finds the best boundary(hyperplane) that separates classes.\n",
    "\n",
    "c.2. Works well in high-dimensional spaces.\n",
    "\n",
    "c.3. Effective for small- to medium-sized datasets.\n",
    "\n",
    "d. K-Nearest Neighbors(KNN):\n",
    "\n",
    "d.1. Classifies a data point based on the majority class of its k nearest neighbors.\n",
    "\n",
    "d.2. Simple, non-parametric.\n",
    "\n",
    "d.3. Slower on large datasets and sensitive to feature scaling.\n",
    "\n",
    "e. Naive Bayes:\n",
    "\n",
    "e.1. Based on Bayes’ Theorem and the assumption of feature independence.\n",
    "\n",
    "e.2. Fast and effective, especially for text classification.\n",
    "\n",
    "e.3. Performs well with small datasets and high-dimensional data.\n",
    "\n",
    "f. Gradient Boosting Machines(e.g., XGBoost, LightGBM, CatBoost):\n",
    "\n",
    "f.1. Ensemble method that builds trees sequentially to correct previous errors.\n",
    "\n",
    "f.2. High accuracy, handles mixed feature types, robust to outliers.\n",
    "\n",
    "f.3. More complex and resource-intensive than logistic regression.\n",
    "\n",
    "g. Neural Networks:\n",
    "\n",
    "g.1. Can model highly nonlinear and complex relationships.\n",
    "\n",
    "g.2. Requires large datasets and tuning.\n",
    "\n",
    "g.3. Suitable for deep learning tasks(images, text, etc.).\n",
    "\n",
    "11. What are Classification Evaluation Metrics?\n",
    "\n",
    "-> Classification evaluation metrics help assess how well a classification model performs. They are crucial for comparing models and making improvements.\n",
    "\n",
    "a. Accuracy:\n",
    "\n",
    "a.1. Definition: The ratio of correctly predicted observations to total observations.\n",
    "\n",
    "Accuracy=(TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "​\n",
    "a.2. Good for: Balanced datasets\n",
    "\n",
    "a.3. Misleading for: Imbalanced datasets\n",
    "\n",
    "b. Precision:\n",
    "\n",
    "b.1. Definition: Out of all predicted positive cases, how many were actually positive.\n",
    "\n",
    "Precision=TP/(TP+FP)\n",
    "​\n",
    "\n",
    "b.2. High Precision=Few False Positives\n",
    "\n",
    "c. Recall(Sensitivity or True Positive Rate):\n",
    "\n",
    "c.1. Definition: Out of all actual positive cases, how many did we correctly predict.\n",
    "\n",
    "Recall=TP/(TP+FN)\n",
    "\n",
    "​ \n",
    "c.2. High Recall=Few False Negatives\n",
    "\n",
    "d. F1 Score:\n",
    "\n",
    "d.1. Definition: Harmonic mean of precision and recall. Useful when there's an imbalance.\n",
    "\n",
    "F1 Score=`(2*Precision*Recall)/(Precision+Recall)`\n",
    "​\n",
    " \n",
    "e. Specificity(True Negative Rate):\n",
    "\n",
    "e.1. Definition: Out of all actual negatives, how many were correctly predicted.\n",
    "\n",
    "Specificity=TN/(TN+FP)\n",
    "\n",
    "​\n",
    "f. Confusion Matrix:\n",
    "\n",
    "f.1. A matrix showing counts of:\n",
    "\n",
    "True Positives(TP)\n",
    "True Negatives(TN)\n",
    "False Positives(FP)\n",
    "False Negatives(FN)\n",
    "\n",
    "g. ROC Curve and AUC(Area Under Curve):\n",
    "\n",
    "g.1. ROC Curve: Plots TPR(Recall) vs. FPR.\n",
    "\n",
    "g.2. AUC: Measures the area under ROC; higher is better(max = 1).\n",
    "\n",
    "h. Log Loss(Cross-Entropy Loss):\n",
    "\n",
    "h.1. Penalizes false confident predictions.\n",
    "\n",
    "h.2. Lower log loss=better calibrated probabilities.\n",
    "\n",
    "12. How does class imbalance affect Logistic Regression?\n",
    "\n",
    "-> Class imbalance occurs when one class (e.g., negative) significantly outnumbers the other(e.g., positive) in a dataset. This can negatively impact the performance of logistic regression and other classification models.\n",
    "\n",
    "Impact of Class Imbalance on Logistic Regression:\n",
    "\n",
    "a. Bias Toward Majority Class: Model may predict only the majority class to maximize accuracy.\n",
    "\n",
    "b. Low Recall for Minority Class: Misses important cases(e.g., fraud, disease).\n",
    "\n",
    "c. Misleading Accuracy: High accuracy can be deceptive; doesn't reflect poor minority class detection.\n",
    "\n",
    "13. What is Hyperparameter Tuning in Logistic Regression?\n",
    "\n",
    "-> Hyperparameter tuning is the process of finding the best combination of external settings(not learned from data) that improve a model’s performance.\n",
    "\n",
    "Key Hyperparameters in Logistic Regression(scikit-learn):\n",
    "\n",
    "a. C(Inverse of regularization strength):\n",
    "\n",
    "a.1. Lower C -> stronger regularization(simpler model)\n",
    "\n",
    "a.2. Higher C -> weaker regularization (more flexible)\n",
    "\n",
    "a.3. Typical values to try: [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "b. penalty:\n",
    "\n",
    "b.1. Type of regularization:\n",
    "\n",
    "'l2': Ridge(default)\n",
    "\n",
    "'l1': Lasso(only with certain solvers)\n",
    "\n",
    "'elasticnet': Combo of L1 and L2\n",
    "\n",
    "'none': No regularization\n",
    "\n",
    "c. solver:\n",
    "\n",
    "c.1. Optimization algorithm: 'liblinear', 'saga', 'lbfgs', 'newton-cg'\n",
    "\n",
    "c.2. Some solvers support only specific penalties.\n",
    "\n",
    "14. What are different solvers in Logistic Regression? Which one should be used?\n",
    "\n",
    "-> In Logistic Regression, the solver determines the optimization algorithm used to estimate the model’s coefficients. Different solvers have different strengths and support various regularization penalties. The most commonly used solver is lbfgs, which is a fast and stable algorithm suitable for both binary and multinomial classification, but it only supports L2 regularization. liblinear is another popular solver that works well for small datasets and supports both L1 and L2 penalties, making it useful when feature selection is desired. For large-scale datasets, solvers like sag and saga are recommended due to their efficiency with large numbers of samples and features. saga is especially versatile as it supports L1, L2, and elastic net penalties and works well for both dense and sparse data. newton-cg is an alternative to lbfgs for multiclass problems and is more accurate in some large-data scenarios.\n",
    "\n",
    "In general, use lbfgs as a good default with L2 regularization, liblinear if you need L1 and have a small dataset, and saga for large datasets or when using L1 or elastic net penalties.\n",
    "\n",
    "15. How is Logistic Regression extended for multiclass classification?\n",
    "\n",
    "-> Logistic Regression is inherently a binary classifier, but it can be extended to handle multiclass classification using two main strategies:\n",
    "\n",
    "a. One-vs-Rest(OvR)/ One-vs-All:\n",
    "\n",
    "a.1. How it works: Trains one binary classifier per class. Each classifier distinguishes one class vs. all others.\n",
    "\n",
    "a.2. Prediction: The class with the highest predicted probability is chosen.\n",
    "\n",
    "a.3. Supported by default in scikit-learn(multi_class='ovr').\n",
    "\n",
    "b. Multinomial(Softmax Regression):\n",
    "\n",
    "b.1. How it works: Generalizes logistic regression by using the softmax function instead of sigmoid.\n",
    "\n",
    "b.2. Prediction: Directly models probabilities for all classes simultaneously.\n",
    "\n",
    "b.3. More accurate than OvR when classes are not linearly separable.\n",
    "\n",
    "b.4. Supported in scikit-learn with multi_class='multinomial' and solver like 'lbfgs' or 'saga'.\n",
    "\n",
    "16. What are the advantages and disadvantages of Logistic Regression?\n",
    "\n",
    "-> Advantages and disadvantages of Logistic Regression:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "a. Simple and Interpretable: \n",
    "\n",
    "a.1. Easy to implement and understand.\n",
    "\n",
    "a.2. Coefficients show the influence of each feature.\n",
    "\n",
    "b. Fast and Efficient:\n",
    "\n",
    "b.1. Low computational cost.\n",
    "\n",
    "b.2. Works well with linearly separable data.\n",
    "\n",
    "c. Probabilistic Output:\n",
    "\n",
    "Predicts probabilities(not just labels), useful in many applications(e.g., risk scoring).\n",
    "\n",
    "d. Regularization Support:\n",
    "\n",
    "Easily supports L1, L2, and Elastic Net regularization.\n",
    "\n",
    "e. Works Well with Small Datasets:\n",
    "\n",
    "Unlike complex models, logistic regression doesn't need large amounts of data.\n",
    "\n",
    "f. Widely Used and Well-Studied:\n",
    "\n",
    "Strong theoretical foundation, many tools and libraries support it.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "a. Assumes Linear Relationship(in log-odds):\n",
    "\n",
    "May perform poorly when the relationship between features and outcome is nonlinear.\n",
    "\n",
    "b. Not Ideal for Complex Patterns:\n",
    "\n",
    "Struggles with high-dimensional or unstructured data(e.g., images, text).\n",
    "\n",
    "c. Sensitive to Outliers:\n",
    "\n",
    "Outliers can distort predictions unless regularized or scaled properly.\n",
    "\n",
    "d. Requires Feature Engineering:\n",
    "\n",
    "Performance depends heavily on good preprocessing and feature selection.\n",
    "\n",
    "e. Limited to Binary or Multiclass Tasks:\n",
    "\n",
    "Cannot handle regression or ranking tasks; requires extensions for multiclass.\n",
    "\n",
    "17. What are some use cases of Logistic Regression?\n",
    "\n",
    "-> Common use cases of Logistic Regression are:\n",
    "\n",
    "Logistic Regression is widely used for binary and multiclass classification tasks across many domains due to its simplicity and interpretability.\n",
    "\n",
    "a. Healthcare:\n",
    "\n",
    "a.1. Disease Prediction: e.g., Predicting whether a patient has diabetes, heart disease, or cancer(Yes/No).\n",
    "\n",
    "a.2. Hospital Readmission: Predicting if a patient is likely to be readmitted within 30 days.\n",
    "\n",
    "b. Finance: \n",
    "\n",
    "b.1. Credit Scoring/ Loan Default: Predicting if a borrower will default on a loan or not.\n",
    "\n",
    "b.2. Fraud Detection: Classifying whether a transaction is fraudulent.\n",
    "\n",
    "c. Marketing:\n",
    "\n",
    "c.1. Customer Churn Prediction: Will a customer leave the service(churn=1).\n",
    "\n",
    "c.2. Email Campaign Effectiveness: Predict if a user will click on or respond to a marketing email.\n",
    "\n",
    "d. HR/ Employee Analytics:\n",
    "\n",
    "d.1. Employee Attrition Prediction: Will an employee resign within the next few months.\n",
    "\n",
    "e. Social Media/ Tech:\n",
    "\n",
    "e.1. Spam Detection: Classify emails or messages as spam or not.\n",
    "\n",
    "e.2. Content Recommendation Click Prediction: Predict if a user will click on a recommended article or ad.\n",
    "\n",
    "f. Manufacturing/ Operations:\n",
    "\n",
    "f.1. Quality Control: Will a product pass inspection(pass/fail).\n",
    "\n",
    "f.2. Predictive Maintenance: Will a machine fail in the near future.\n",
    "\n",
    "18. What is the difference between Softmax Regression and Logistic Regression?\n",
    "\n",
    "-> Difference between Logistic Regression and Softmax Regression are:\n",
    "\n",
    "a. Output Type:\n",
    "\n",
    "Logistic Regression:\n",
    "\n",
    "a.1. Used for binary classification(i.e., two classes: 0 or 1).\n",
    "\n",
    "a.2. Predicts the probability that a sample belongs to class 1 using the sigmoid function.\n",
    "\n",
    "Softmax Regression(Multinomial Logistic Regression):\n",
    "\n",
    "a.1. Used for multiclass classification (i.e., more than two classes).\n",
    "\n",
    "a.2. Predicts the probability of each class using the softmax function.\n",
    "\n",
    "b. Activation Function:\n",
    "\n",
    "Logistic Regression: Uses sigmoid function\n",
    " \n",
    "Softmax Regression: Uses softmax function\n",
    "\n",
    "c. Number of Classifiers:\n",
    "\n",
    "Logistic Regression: One classifier for binary outcome\n",
    "\n",
    "Softmax Regression: One model that directly predicts probabilities for all classes simultaneously\n",
    "\n",
    "d. Use Case:\n",
    "\n",
    "Use Logistic Regression for binary problems (e.g., spam vs. not spam).\n",
    "\n",
    "Use Softmax Regression for multiclass problems (e.g., digit classification: 0–9).\n",
    "\n",
    "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
    "\n",
    "-> Choosing between One-vs-Rest(OvR) and Softmax for Multiclass Classification:\n",
    "\n",
    "a. Problem Complexity & Class Overlap:\n",
    "\n",
    "Use OvR if:\n",
    "\n",
    "a.1. Classes are well-separated\n",
    "\n",
    "a.2. We want to interpret how each class behaves independently\n",
    "\n",
    "a.3. We need faster training and simpler models\n",
    "\n",
    "Use Softmax (Multinomial) if:\n",
    "\n",
    "a.1. Classes are not clearly separable\n",
    "\n",
    "a.2. We want a single global model that considers all classes simultaneously\n",
    "\n",
    "a.3. We care about probability calibration across all classes\n",
    "\n",
    "b. Model Accuracy:\n",
    "\n",
    "Softmax usually gives better accuracy when:\n",
    "\n",
    "b.1. Classes are correlated\n",
    "\n",
    "b.2. Dataset is large\n",
    "\n",
    "b.3. Global optimization improves performance\n",
    "\n",
    "OvR may perform similarly or better when:\n",
    "\n",
    "b.1. Data is small or imbalanced\n",
    "\n",
    "b.2. Class boundaries are simple\n",
    "\n",
    "c. Interpretability and Debugging:\n",
    "\n",
    "OvR is more interpretable because:\n",
    "\n",
    "c.1. Each classifier handles one class vs all others\n",
    "\n",
    "c.2. Easier to debug misclassifications\n",
    "\n",
    "d. Library Support(scikit-learn):\n",
    "\n",
    "LogisticRegression(multi_class='ovr'): One-vs-Rest\n",
    "\n",
    "LogisticRegression(multi_class='multinomial'): Softmax(needs solver='lbfgs' or 'saga')\n",
    "\n",
    "20. How do we interpret coefficients in Logistic Regression?\n",
    "\n",
    "-> In logistic regression, each coefficient βj represents the effect of the corresponding feature xj on the log-odds of the outcome.\n",
    "\n",
    "Logistic Regression Equation:\n",
    "\n",
    "`log(p/(1-p))=β0+β1*x1+β2*x2+...+βn*xn`\n",
    "\n",
    "Where:\n",
    "\n",
    "p=probability of the positive class(y=1)\n",
    "\n",
    "βj=coefficient for feature xj\n",
    "\n",
    "\n",
    "a. Coefficients are in log-odds: Each βj represents the change in log-odds of y=1 for a one-unit increase in xj, keeping all else constant.\n",
    "\n",
    "b. Convert to Odds Ratio for easier interpretation:   \n",
    "\n",
    "odds_ratio=exp(βj)\n",
    "\n",
    "b.1. If odds_ratio>1: xj increases odds of positive class\n",
    "\n",
    "b.2. If odds_ratio<1: xj decreases odds of positive class\n",
    "\n",
    "b.3. If odds_ratio=1: xj has no effect\n",
    "\n",
    "Example:\n",
    "\n",
    "β1=0.7 -> odds_ratio=exp(0.7)≈2.01\n",
    "\n",
    "A 1-unit increase in x1 multiplies the odds of y=1 by ~2.01\n",
    "\n",
    "c. Sign of Coefficient:\n",
    "\n",
    "c.1. Positive β -> increases log-odds -> increases probability of y=1\n",
    "\n",
    "c.2. Negative β -> decreases log-odds -> decreases probability of y=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d846cc54",
   "metadata": {},
   "source": [
    "Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16050a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is: 0.95\n"
     ]
    }
   ],
   "source": [
    "# 1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(f\"Model accuracy is: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2177fe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is: 0.96\n"
     ]
    }
   ],
   "source": [
    "# 2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression(penalty='l1',solver='liblinear')\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(f\"Model accuracy is: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39a20926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is: 0.95\n",
      "Model coefficients: [[ 1.32158992  0.27853421  0.05242603 -0.00233866 -0.05146569 -0.24332956\n",
      "  -0.33668728 -0.14475048 -0.07690998 -0.01552786  0.04393003  0.62012846\n",
      "   0.05702203 -0.05438627 -0.00485959 -0.04871799 -0.06778173 -0.01854911\n",
      "  -0.01816777 -0.00424533  1.3788316  -0.442407   -0.13676157 -0.02683422\n",
      "  -0.09133262 -0.7313164  -0.90446904 -0.2666783  -0.24869424 -0.06825443]]\n"
     ]
    }
   ],
   "source": [
    "# 3.  Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression(penalty='l2')\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(f\"Model accuracy is: {accuracy:.2f}\")\n",
    "print(\"Model coefficients:\", model.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb826841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is: 0.95\n"
     ]
    }
   ],
   "source": [
    "# 4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression(penalty='elasticnet',solver='saga',l1_ratio=0.5)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(f\"Model accuracy is: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0b12cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is: 0.97\n"
     ]
    }
   ],
   "source": [
    "# 5.  Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_iris()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression(multi_class='ovr')\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(f\"Model accuracy is: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e703fe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 100, 'penalty': 'l2'}\n",
      "Model accuracy is: 0.96\n"
     ]
    }
   ],
   "source": [
    "# 6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression()\n",
    "param_grid={\n",
    "    'C':[0.01,0.1,1,10,100],\n",
    "    'penalty':['l1','l2','elasticnet']\n",
    "}\n",
    "grid_search=GridSearchCV(model,param_grid,cv=5,scoring='accuracy')\n",
    "grid_search.fit(X_train,y_train)\n",
    "best_params=grid_search.best_params_\n",
    "best_model=grid_search.best_estimator_\n",
    "y_pred=best_model.predict(X_test)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Model accuracy is: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "581dd061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy from Stratified K-Fold Cross-Validation: 0.94\n"
     ]
    }
   ],
   "source": [
    "# 7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "skf=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "model=LogisticRegression()\n",
    "accuracies=[]\n",
    "for train_index,test_index in skf.split(X, Y):\n",
    "    X_train,X_test=X.iloc[train_index],X.iloc[test_index]\n",
    "    y_train,y_test=Y[train_index],Y[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred=model.predict(X_test)\n",
    "    accuracy=accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "average_accuracy=sum(accuracies)/len(accuracies)\n",
    "print(f\"Average accuracy from Stratified K-Fold Cross-Validation: {average_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5ec4005a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is: 0.75\n"
     ]
    }
   ],
   "source": [
    "# 8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=pd.read_csv('diabetes.csv') \n",
    "X=data.iloc[:,:-1]\n",
    "Y=data.iloc[:,-1]\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(f\"Model accuracy is: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b3ff558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 10}\n",
      "Model accuracy is: 0.97\n"
     ]
    }
   ],
   "source": [
    "# 9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression()\n",
    "param_distributions={\n",
    "    'C':[0.01,0.1,1,10,100],\n",
    "    'penalty':['l1','l2','elasticnet'],\n",
    "    'solver':['liblinear','saga']\n",
    "}\n",
    "random_search=RandomizedSearchCV(model,param_distributions=param_distributions,n_iter=10,cv=5,scoring='accuracy')\n",
    "random_search.fit(X_train,y_train)\n",
    "best_params=random_search.best_params_\n",
    "best_model=random_search.best_estimator_\n",
    "y_pred=best_model.predict(X_test)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Model accuracy is: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3cb4b8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is: 1.00\n"
     ]
    }
   ],
   "source": [
    "# 10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_iris()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=OneVsOneClassifier(LogisticRegression())\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(f\"Model accuracy is: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25837253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for binary classification:\n",
      "[[38  5]\n",
      " [ 1 70]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "cm=confusion_matrix(y_test, y_pred)\n",
    "disp=ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)\n",
    "disp.plot(cmap='Blues')\n",
    "print(f\"Confusion matrix for binary classification:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c810582a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.93\n",
      "Recall: 0.99\n",
      "F1-Score: 0.96\n"
     ]
    }
   ],
   "source": [
    "# 12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "precision=precision_score(y_test,y_pred)\n",
    "recall=recall_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1aa87cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy with class weights: 0.70\n"
     ]
    }
   ],
   "source": [
    "# 13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=pd.read_csv('diabetes.csv')\n",
    "X=data.drop('Outcome',axis=1)\n",
    "Y=data['Outcome']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression(class_weight='balanced')\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(f\"Model accuracy with class weights: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3fc30953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on Titanic dataset is: 0.81\n"
     ]
    }
   ],
   "source": [
    "# 14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data=sns.load_dataset('titanic')\n",
    "data=data.drop(columns=['deck','embark_town','alive','class','who','adult_male','alone','embarked'])\n",
    "data['age'].fillna(data['age'].median(),inplace=True)\n",
    "data['sex']=data['sex'].map({\n",
    "    'male':1,\n",
    "    'female':0\n",
    "})\n",
    "X=data.drop(columns=['survived'])\n",
    "Y=data['survived']\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(f\"Model accuracy on Titanic dataset is: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "596ead06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy without scaling: 0.95\n",
      "Model accuracy with scaling: 0.97\n"
     ]
    }
   ],
   "source": [
    "# 15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "accuracy_without_scaling=accuracy_score(y_test,y_pred)\n",
    "print(f\"Model accuracy without scaling: {accuracy_without_scaling:.2f}\")\n",
    "\n",
    "scaler=StandardScaler()\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)\n",
    "model.fit(X_train_scaled,y_train)\n",
    "y_pred_scaled=model.predict(X_test_scaled)\n",
    "accuracy_with_scaling=accuracy_score(y_test,y_pred_scaled)\n",
    "print(f\"Model accuracy with scaling: {accuracy_with_scaling:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4cc8c4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score: 0.93\n"
     ]
    }
   ],
   "source": [
    "# 16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "roc_auc=roc_auc_score(y_test, y_pred)\n",
    "print(f\"ROC-AUC score: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32b96a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is: 0.96\n"
     ]
    }
   ],
   "source": [
    "# 17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression(C=0.5)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(f\"Model accuracy is: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aab86d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important features based on model coefficients:\n",
      "worst radius               1.378832\n",
      "mean radius                1.321590\n",
      "texture error              0.620128\n",
      "mean texture               0.278534\n",
      "perimeter error            0.057022\n",
      "mean perimeter             0.052426\n",
      "radius error               0.043930\n",
      "mean area                 -0.002339\n",
      "fractal dimension error   -0.004245\n",
      "smoothness error          -0.004860\n",
      "mean fractal dimension    -0.015528\n",
      "symmetry error            -0.018168\n",
      "concave points error      -0.018549\n",
      "worst area                -0.026834\n",
      "compactness error         -0.048718\n",
      "mean smoothness           -0.051466\n",
      "area error                -0.054386\n",
      "concavity error           -0.067782\n",
      "worst fractal dimension   -0.068254\n",
      "mean symmetry             -0.076910\n",
      "worst smoothness          -0.091333\n",
      "worst perimeter           -0.136762\n",
      "mean concave points       -0.144750\n",
      "mean compactness          -0.243330\n",
      "worst symmetry            -0.248694\n",
      "worst concave points      -0.266678\n",
      "mean concavity            -0.336687\n",
      "worst texture             -0.442407\n",
      "worst compactness         -0.731316\n",
      "worst concavity           -0.904469\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 18.  Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "coefficients=model.coef_[0]\n",
    "important_features=pd.Series(coefficients,index=X.columns).sort_values(ascending=False)\n",
    "print(\"Important features based on model coefficients:\")\n",
    "print(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e4ee8f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa Score: 0.89\n"
     ]
    }
   ],
   "source": [
    "# 19.  Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "cohen_kappa=cohen_kappa_score(y_test,y_pred)\n",
    "print(f\"Cohen's Kappa Score: {cohen_kappa:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28ddb710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "precision,recall,_=precision_recall_curve(y_test,y_pred)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(recall,precision,marker='.')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a13a1ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy with liblinear solver: 0.96\n",
      "Model accuracy with saga solver: 0.95\n",
      "Model accuracy with lbfgs solver: 0.95\n"
     ]
    }
   ],
   "source": [
    "# 21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "solvers=['liblinear','saga','lbfgs']\n",
    "for solver in solvers:\n",
    "    model=LogisticRegression(solver=solver)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred=model.predict(X_test)\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    print(f\"Model accuracy with {solver} solver: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "246a164f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC) is: 0.89\n"
     ]
    }
   ],
   "source": [
    "# 22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "mcc=matthews_corrcoef(y_test,y_pred)\n",
    "print(f\"Matthews Correlation Coefficient (MCC) is: {mcc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "99a71285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy without scaling: 0.95\n",
      "Model accuracy with scaling: 0.97\n"
     ]
    }
   ],
   "source": [
    "# 23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "accuracy_without_scaling=accuracy_score(y_test,y_pred)\n",
    "print(f\"Model accuracy without scaling: {accuracy_without_scaling:.2f}\")\n",
    "\n",
    "scaler=StandardScaler()\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)\n",
    "model.fit(X_train_scaled,y_train)\n",
    "y_pred_scaled=model.predict(X_test_scaled)\n",
    "accuracy_with_scaling=accuracy_score(y_test,y_pred_scaled)\n",
    "print(f\"Model accuracy with scaling: {accuracy_with_scaling:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9a72c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal C value: 100\n"
     ]
    }
   ],
   "source": [
    "# 24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "c_values=[0.01,0.1,1,10,100]\n",
    "best_score=0\n",
    "best_c=0\n",
    "for c in c_values:\n",
    "    model=LogisticRegression(C=c)\n",
    "    scores=cross_val_score(model,X_train,y_train,cv=5,scoring='accuracy')\n",
    "    mean_score=scores.mean()\n",
    "    if mean_score>best_score:\n",
    "        best_score=mean_score\n",
    "        best_c=c\n",
    "print(f\"Optimal C value: {best_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "155e865c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy after loading: 0.95\n"
     ]
    }
   ],
   "source": [
    "# 25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=load_breast_cancer()\n",
    "X=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "Y=data.target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "joblib.dump(model,'logistic_regression_model.joblib')\n",
    "loaded_model=joblib.load('logistic_regression_model.joblib')\n",
    "y_pred=loaded_model.predict(X_test)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(f\"Model accuracy after loading: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
